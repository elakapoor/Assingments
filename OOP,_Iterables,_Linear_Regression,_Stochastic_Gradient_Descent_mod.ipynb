{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison how Gradient Descent and Stochastic Gradient Descent differ in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence (convergence speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case (when to use which algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.\n",
    "\n",
    "Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Their difference from Optimization point of view. Here you should use the following terms: epoch, total amount of iterations, batch size, minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch, thumb rule for selecting the size of mini-batch is in power of 2 like 32, 64, 128 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Descent__<br>\n",
    "This is an iterative optimization algorithm for finding the minimum of a function. The algorithm takes steps proportional to the negative gradient of the function at the current point. <br>\n",
    "\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\frac{\\partial }{\\partial \\theta_j}J(\\theta) $$<br>\n",
    "\n",
    "Here,<br> \n",
    "$\\theta_j$ corresponds to the parameter, $\\alpha$ is the learning rate that is the step size multiplied by the derivative of the function by which to move on the loss function curve toward the minima.<br>\n",
    "\n",
    "In deep learning neural networks are trained by defining a loss function and optimizing the parameters of the network to obtain the minimum of the function. The optimization is done using the gradient descent algorithm which operates in these two steps:<br>\n",
    "1. Compute the slope (gradient) that is first order derivative of the function at the current point.\n",
    "2. Move in the opposite direction of the slope increase from the current point by the computed amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stochastic Gradient Descent__<br>\n",
    "In this method one training sample (example) is passed through the neural network at a time and the parameters (weights) of each layer are updated with the computed gradient. So, at a time a single training sample is passed through the network and its corresponding loss is computed. The parameters of all the layers of the network are updated after every training sample. For example, if the training set contains 100 samples then the parameters are updated 100 times that is one time after every individual example is passed through the network. Following is the gradient descent equation and for stochastic gradient descent it is iterated over ’n’ times for ’n’ training samples in the training set.<br>\n",
    "\n",
    "__Advantages of Stochastic Gradient Descent__<br>\n",
    "1. It is easier to fit into memory due to a single training sample being processed by the network\n",
    "2. It is computationally fast as only one sample is processed at a time\n",
    "3. For larger datasets it can converge faster as it causes updates to the parameters more frequently\n",
    "4. Due to frequent updates the steps taken towards the minima of the loss function have oscillations which can help getting out of local minimums of the loss function (in case the computed position turns out to be the local minimum)\n",
    "\n",
    "<br>__Disadvantages of Stochastic Gradient Descent__<br>\n",
    "1. Due to frequent updates the steps taken towards the minima are very noisy. This can often lead the gradient descent into other directions.\n",
    "2. Also, due to noisy steps it may take longer to achieve convergence to the minima of the loss function\n",
    "3. Frequent updates are computationally expensive due to using all resources for processing one training sample at a time\n",
    "4. It loses the advantage of vectorized operations as it deals with only a single example at a time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch Gradient Descent__<br>\n",
    "The concept of carrying out gradient descent is the same as stochastic gradient descent. The difference is that instead of updating the parameters of the network after computing the loss of every training sample in the training set, the parameters are updated once that is after all the training examples have been passed through the network. For example, if the training dataset contains 100 training examples then the parameters of the neural network are updated once. The equation in Figure2 is iterated over only once.<br>\n",
    "\n",
    "__Advantages of Batch Gradient Descent__<br>\n",
    "Less oscillations and noisy steps taken towards the global minima of the loss function due to updating the parameters by computing the average of all the training samples rather than the value of a single sample\n",
    "1. It can benefit from the vectorization which increases the speed of processing all training samples together\n",
    "2. It produces a more stable gradient descent convergence and stable error gradient than stochastic gradient descent\n",
    "3. It is computationally efficient as all computer resources are not being used to process a single sample rather are    being used for all training samples<br>\n",
    "\n",
    "__Disadvantages of Batch Gradient Descent__<br>\n",
    "1. Sometimes a stable error gradient can lead to a local minima and unlike stochastic gradient descent no noisy steps are there to help get out of the local minima\n",
    "2. The entire training set can be too large to process in the memory due to which additional memory might be needed\n",
    "3. Depending on computer resources it can take too long for processing all the training samples as a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mini Batch Gradient Descent Batch : A Compromise__<br>\n",
    "This is a mixture of both stochastic and batch gradient descent. The training set is divided into multiple groups called batches. Each batch has a number of training samples in it. At a time a single batch is passed through the network which computes the loss of every sample in the batch and uses their average to update the parameters of the neural network. For example, say the training set has 100 training examples which is divided into 5 batches with each batch containing 20 training examples. This means that the equation in __figure2__ will be iterated over 5 times (number of batches).<br>\n",
    "This ensures the following advantages of both stochastic and batch gradient descent are used due to which Mini Batch Gradient Descent is most commonly used in practice.<br>\n",
    "1. Easily fits in the memory\n",
    "2. It is computationally efficient\n",
    "3. Benefit from vectorization\n",
    "4. If stuck in local minimums, some noisy steps can lead the way out of them\n",
    "5. Average of the training samples produces stable error gradients and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
